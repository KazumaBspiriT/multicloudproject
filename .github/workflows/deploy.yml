name: Multi-Cloud Deployment Pipeline

on:
  workflow_dispatch:
    inputs:
      target_cloud:
        description: 'Target Cloud (aws, azure, gcp)'
        required: true
        default: 'aws'
      deployment_mode:
        description: 'Deployment Mode (k8s, container, or static)'
        required: true
        default: 'k8s'
      app_image:
        description: 'Container Image (Docker Hub URI for GCP/Azure/General)'
        required: false
        default: 'nginx:latest'
      app_image_aws:
        description: 'AWS Image URI (ECR Public/Private). If different from app_image.'
        required: false
        default: ''
      domain_name:
        description: 'Custom Domain Name (optional)'
        required: false
        default: ''

jobs:
  deploy:
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: us-east-2
      TF_IN_AUTOMATION: true
      TF_INPUT: false
      ENABLE_NAT_GATEWAY: false # Default to false to save costs

    steps:
      - uses: actions/checkout@v4

      - name: Start Provisioning Timer
        id: timer_start
        run: echo "start=$(date +%s)" >> "$GITHUB_OUTPUT"

      # AWS creds are required for S3 State Backend, even if deploying to Azure/GCP
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # GCP Authentication (Required for Terraform Provider init)
      - name: Configure GCP Credentials
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup Google Cloud SDK
        if: ${{ contains(inputs.target_cloud, 'gcp') }}
        uses: google-github-actions/setup-gcloud@v2

      # Ensure awscli is available for S3 sync in the null_resource
      - name: Check AWS CLI
        if: ${{ inputs.target_cloud == 'aws' && inputs.deployment_mode == 'static' }}
        run: |
          if ! command -v aws >/dev/null 2>&1; then
            sudo apt-get update -y
            sudo apt-get install -y awscli
          fi
          aws --version

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Get AWS Account ID
        id: get_account
        run: echo "account_id=$(aws sts get-caller-identity --query Account --output text)" >> "$GITHUB_OUTPUT"

      - name: Terraform Init
        run: |
          terraform init -input=false \
            -backend-config="bucket=multicloud-tf-state-${{ steps.get_account.outputs.account_id }}" \
            -backend-config="key=global/s3/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="dynamodb_table=terraform-locks" \
            -backend-config="encrypt=true"

      - name: Terraform Plan
        id: plan
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
        run: |
          # Convert comma-separated string to JSON list
          CLOUDS_INPUT="${{ inputs.target_cloud }}"
          TF_CLOUDS_LIST="[\"$(echo $CLOUDS_INPUT | sed 's/,/\",\"/g')\"]"
          
          terraform plan \
            -input=false \
            -var="target_clouds=$TF_CLOUDS_LIST" \
            -var="deployment_mode=${{ inputs.deployment_mode }}" \
            -var="app_image=${{ inputs.app_image }}" \
            -var="app_image_aws=${{ inputs.app_image_aws }}" \
            -var="domain_name=${{ inputs.domain_name }}" \
            -var="enable_nat_gateway=${{ env.ENABLE_NAT_GATEWAY }}" \
            -out=tfplan.out

      - name: Terraform Apply (Provisioning)
        id: apply
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
        run: terraform apply -input=false tfplan.out

      - name: Export Terraform outputs safely
        id: tf_outputs
        shell: bash
        run: |
          set -euo pipefail
          CLOUD="${{ inputs.target_cloud }}"
          MODE="${{ inputs.deployment_mode }}"

          # Get all outputs as JSON; if none, keep it empty JSON
          OUT_JSON="$(terraform output -json || echo '{}')"

          # Helper to read a key if present
          jget() { echo "$OUT_JSON" | jq -r "$1 // empty"; }

          # AWS + k8s: guarded EKS outputs
          if [ "$CLOUD" = "aws" ] && [ "$MODE" = "k8s" ]; then
            CLUSTER_ENDPOINT="$(jget '.eks_cluster_endpoint.value')"
            EKS_CLUSTER_NAME="$(jget '.eks_cluster_name.value')"
            EKS_VPC_ID="$(jget '.eks_vpc_id.value')"
            EKS_LB_ROLE_ARN="$(jget '.eks_lb_role_arn.value')"
            ACM_CERT_ARN="$(jget '.eks_acm_certificate_arn.value')"
            
            if [ -n "$CLUSTER_ENDPOINT" ]; then
              echo "CLUSTER_ENDPOINT=$CLUSTER_ENDPOINT" >> "$GITHUB_ENV"
              echo "cluster_endpoint=$CLUSTER_ENDPOINT" >> "$GITHUB_OUTPUT"
            fi
            if [ -n "$EKS_CLUSTER_NAME" ]; then echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> "$GITHUB_ENV"; fi
            if [ -n "$EKS_VPC_ID" ]; then echo "EKS_VPC_ID=$EKS_VPC_ID" >> "$GITHUB_ENV"; fi
            if [ -n "$EKS_LB_ROLE_ARN" ]; then echo "EKS_LB_ROLE_ARN=$EKS_LB_ROLE_ARN" >> "$GITHUB_ENV"; fi
            if [ -n "$ACM_CERT_ARN" ]; then echo "ACM_CERT_ARN=$ACM_CERT_ARN" >> "$GITHUB_ENV"; fi
          fi

          # GCP + k8s: guarded GKE outputs
          if [ "$CLOUD" = "gcp" ] && [ "$MODE" = "k8s" ]; then
            GKE_NAME="$(jget '.gke_cluster_name.value')"
            GKE_ENDPOINT="$(jget '.gke_cluster_endpoint.value')"
            echo "GKE_NAME=$GKE_NAME" >> "$GITHUB_ENV"
            if [ -n "$GKE_ENDPOINT" ]; then
               echo "CLUSTER_ENDPOINT=$GKE_ENDPOINT" >> "$GITHUB_ENV"
               echo "cluster_endpoint=$GKE_ENDPOINT" >> "$GITHUB_OUTPUT"
            fi
          fi

          # Static mode (AWS or GCP)
          if [ "$MODE" = "static" ]; then
            WEBSITE_URL="$(jget '.website_endpoint.value')"
            if [ -n "$WEBSITE_URL" ]; then
              echo "WEBSITE_URL=$WEBSITE_URL" >> "$GITHUB_ENV"
              echo "website_url=$WEBSITE_URL" >> "$GITHUB_OUTPUT"
            fi
          fi

          # Container mode
          if [ "$MODE" = "container" ]; then
            CONTAINER_URL="$(jget '.container_url.value')"
            if [ -n "$CONTAINER_URL" ]; then
              echo "CONTAINER_URL=$CONTAINER_URL" >> "$GITHUB_ENV"
              echo "container_url=$CONTAINER_URL" >> "$GITHUB_OUTPUT"
            fi
          fi

      - name: Configure Kubeconfig for GKE
        if: ${{ inputs.target_cloud == 'gcp' && inputs.deployment_mode == 'k8s' }}
        run: |
          # Retrieve credentials using the cluster name from Terraform outputs
          # Assumes 'setup-gcloud' has already configured the project from credentials or default
          gcloud container clusters get-credentials "${{ env.GKE_NAME }}" --region "us-central1"
          # Copy default config to kubeconfig.yaml for Ansible
          cp $HOME/.kube/config ./kubeconfig.yaml

      - name: Measure Provisioning Time
        id: time_provisioning
        run: |
          end=$(date +%s)
          start=${{ steps.timer_start.outputs.start }}
          echo "duration=$((end - start))" >> "$GITHUB_OUTPUT"
          echo "Provisioning Time: $((end - start)) seconds"

      # ---- Ansible only for k8s mode (if you actually have these files) ----
      - name: Setup Python and Ansible
        if: ${{ inputs.deployment_mode == 'k8s' }}
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install kubectl
        if: ${{ inputs.deployment_mode == 'k8s' }}
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Install Helm
        if: ${{ inputs.deployment_mode == 'k8s' }}
        uses: azure/setup-helm@v3
        with:
          version: 'latest'

      - name: Install Ansible + k8s deps
        if: ${{ inputs.deployment_mode == 'k8s' }}
        run: |
          pip install ansible kubernetes openshift jmespath
          ansible-galaxy collection install kubernetes.core community.general

      - name: Debug - List Files
        if: ${{ inputs.deployment_mode == 'k8s' }}
        run: |
          echo "Listing root directory:"
          ls -la
          if [ -f kubeconfig.yaml ]; then
            echo "kubeconfig.yaml found!"
            head -n 5 kubeconfig.yaml
          else
            echo "kubeconfig.yaml NOT found!"
          fi

      - name: Run Ansible (K8s Mode)
        if: ${{ inputs.deployment_mode == 'k8s' }}
        env:
          KUBECONFIG: kubeconfig.yaml
        run: |
          if [ ! -f ansible/inventory.yml ] || [ ! -f ansible/playbook.yml ]; then
            echo "Ansible files not present, skipping."
            exit 0
          fi
          ansible-playbook -i ansible/inventory.yml ansible/playbook.yml \
            --extra-vars "deployment_mode=${{ inputs.deployment_mode }} app_image=${{ inputs.app_image }} domain_name=${{ inputs.domain_name }} target_clouds=${{ inputs.target_cloud }} acm_certificate_arn=${{ env.ACM_CERT_ARN }} eks_cluster_name=${{ env.EKS_CLUSTER_NAME }} eks_vpc_id=${{ env.EKS_VPC_ID }} aws_region=${{ env.AWS_REGION }} eks_lb_role_arn=${{ env.EKS_LB_ROLE_ARN }}"

      - name: Update Route 53 DNS (Auto)
        if: ${{ inputs.deployment_mode == 'k8s' && inputs.target_cloud == 'aws' && inputs.domain_name != '' }}
        env:
          KUBECONFIG: kubeconfig.yaml
          DOMAIN_NAME: ${{ inputs.domain_name }}
          AWS_REGION: ${{ env.AWS_REGION }}
        run: |
          echo "Waiting for Ingress to be assigned a hostname..."
          ALB_HOSTNAME=""
          for i in {1..24}; do
            ALB_HOSTNAME=$(kubectl get ingress -n sample-app portfolio-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            if [[ -n "$ALB_HOSTNAME" ]]; then
              echo "Found ALB Hostname: $ALB_HOSTNAME"
              break
            fi
            sleep 5
          done
          
          if [[ -n "$ALB_HOSTNAME" ]]; then
            # Get Hosted Zone ID (assume we can query it or get from terraform)
            # We can get it from terraform output but we didn't export it to ENV. Let's export it in the 'Export Terraform outputs safely' step first?
            # Or just query AWS since we have creds.
            
            HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='$DOMAIN_NAME.'].Id" --output text | cut -d'/' -f3)
            
            if [[ -n "$HOSTED_ZONE_ID" ]]; then
               # Get ALB Zone ID
               ALB_ZONE_ID=$(aws elbv2 describe-load-balancers --region "$AWS_REGION" --query "LoadBalancers[?DNSName=='$ALB_HOSTNAME'].CanonicalHostedZoneId" --output text)
               
               if [[ -n "$ALB_ZONE_ID" ]]; then
                 echo "Updating Route 53..."
                 cat > change-batch.json <<EOF
          {
            "Comment": "Auto-update Alias record for ALB",
            "Changes": [
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "$DOMAIN_NAME",
                  "Type": "A",
                  "AliasTarget": {
                    "HostedZoneId": "$ALB_ZONE_ID",
                    "DNSName": "$ALB_HOSTNAME",
                    "EvaluateTargetHealth": true
                  }
                }
              },
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "www.$DOMAIN_NAME",
                  "Type": "A",
                  "AliasTarget": {
                    "HostedZoneId": "$ALB_ZONE_ID",
                    "DNSName": "$ALB_HOSTNAME",
                    "EvaluateTargetHealth": true
                  }
                }
              }
            ]
          }
          EOF
                 aws route53 change-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" --change-batch file://change-batch.json
                 echo "DNS Updated."
               fi
            fi
          fi

      # Simple smoke log (you can curl a URL here if you output one for k8s)
      - name: Smoke Log
        run: |
          echo "Cloud: ${{ inputs.target_cloud }}"
          echo "Mode:  ${{ inputs.deployment_mode }}"
          if [ -n "${{ env.WEBSITE_URL }}" ]; then
            echo "Static site endpoint: ${{ env.WEBSITE_URL }}"
          fi
          if [ -n "${{ env.CONTAINER_URL }}" ]; then
            echo "App Runner URL: ${{ env.CONTAINER_URL }}"
          fi
          if [ -n "${{ env.CLUSTER_ENDPOINT }}" ]; then
            echo "EKS API endpoint: ${{ env.CLUSTER_ENDPOINT }}"
          fi

      # Destroy resources ONLY if the pipeline fails (failure() returns true)
      - name: Terraform Destroy (on failure)
        if: failure()
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
        run: |
          CLOUDS_INPUT="${{ inputs.target_cloud }}"
          TF_CLOUDS_LIST="[\"$(echo $CLOUDS_INPUT | sed 's/,/\",\"/g')\"]"
          
          terraform destroy -auto-approve \
            -var="target_clouds=$TF_CLOUDS_LIST" \
            -var="deployment_mode=${{ inputs.deployment_mode }}" \
            -var="app_image=${{ inputs.app_image }}" \
            -var="app_image_aws=${{ inputs.app_image_aws }}" \
            -var="domain_name=${{ inputs.domain_name }}" \
            -var="enable_nat_gateway=${{ env.ENABLE_NAT_GATEWAY }}"

      - name: Publish Metrics
        run: |
          echo "--- FINAL METRICS ---"
          echo "Cloud: ${{ inputs.target_cloud }}, Mode: ${{ inputs.deployment_mode }}"
          echo "Provisioning Time (s): ${{ steps.time_provisioning.outputs.duration }}"
