name: Multi-Cloud Deployment Pipeline

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action to perform (deploy or destroy)'
        required: true
        type: choice
        options:
          - deploy
          - destroy
        default: 'deploy'
      target_cloud:
        description: 'Target Cloud (aws, azure, gcp) - comma-separated for multi-cloud'
        required: true
        default: 'aws'
      deployment_mode:
        description: 'Deployment Mode (k8s, container, or static)'
        required: true
        type: choice
        options:
          - k8s
          - container
          - static
          - all
        default: 'k8s'
      app_image:
        description: 'Container Image (Docker Hub URI for GCP/Azure/General)'
        required: false
        default: 'nginx:latest'
      domain_name:
        description: 'Custom Domain Name (optional)'
        required: false
        default: ''
      enable_nat_gateway:
        description: 'Enable NAT Gateway for AWS EKS (costs ~$32/month)'
        required: false
        type: boolean
        default: false

jobs:
  deploy:
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    env:
      AWS_REGION: us-east-2
      TF_IN_AUTOMATION: true
      TF_INPUT: false
      ENABLE_NAT_GATEWAY: ${{ inputs.enable_nat_gateway }}

    steps:
      - uses: actions/checkout@v4

      - name: Validate Required Secrets
        run: |
          echo "ðŸ” Validating required secrets..."
          MISSING_SECRETS=0
          
          # AWS secrets are ALWAYS required (for S3 backend)
          echo "Checking AWS secrets (required for S3 backend)..."
          if [ -z "${{ secrets.AWS_ACCESS_KEY_ID }}" ]; then
            echo "âŒ Missing: AWS_ACCESS_KEY_ID"
            MISSING_SECRETS=1
          else
            echo "âœ… AWS_ACCESS_KEY_ID is set"
          fi
          
          if [ -z "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ]; then
            echo "âŒ Missing: AWS_SECRET_ACCESS_KEY"
            MISSING_SECRETS=1
          else
            echo "âœ… AWS_SECRET_ACCESS_KEY is set"
          fi
          
          # Check GCP secrets if GCP is in target clouds
          if [[ "${{ inputs.target_cloud }}" == *"gcp"* ]]; then
            echo ""
            echo "Checking GCP secrets (required for GCP deployment)..."
            if [ -z "${{ secrets.GCP_SA_KEY }}" ]; then
              echo "âŒ Missing: GCP_SA_KEY"
              MISSING_SECRETS=1
            else
              # Validate JSON format (if jq is available)
              if command -v jq &> /dev/null; then
                if echo "${{ secrets.GCP_SA_KEY }}" | jq empty 2>/dev/null; then
                  echo "âœ… GCP_SA_KEY is set and valid JSON"
                else
                  echo "âš ï¸  GCP_SA_KEY is set but may not be valid JSON"
                  echo "   Make sure you copied ONLY the JSON content (no extra text)"
                fi
              else
                echo "âœ… GCP_SA_KEY is set"
                echo "   (JSON validation skipped - jq not available)"
              fi
            fi
          fi
          
          # Check Azure secrets if Azure is in target clouds
          if [[ "${{ inputs.target_cloud }}" == *"azure"* ]]; then
            echo ""
            echo "Checking Azure secrets (required for Azure deployment)..."
            if [ -z "${{ secrets.ARM_CLIENT_ID }}" ]; then
              echo "âŒ Missing: ARM_CLIENT_ID"
              MISSING_SECRETS=1
            else
              echo "âœ… ARM_CLIENT_ID is set"
            fi
            
            if [ -z "${{ secrets.ARM_CLIENT_SECRET }}" ]; then
              echo "âŒ Missing: ARM_CLIENT_SECRET"
              MISSING_SECRETS=1
            else
              echo "âœ… ARM_CLIENT_SECRET is set"
            fi
            
            if [ -z "${{ secrets.ARM_SUBSCRIPTION_ID }}" ]; then
              echo "âŒ Missing: ARM_SUBSCRIPTION_ID"
              MISSING_SECRETS=1
            else
              echo "âœ… ARM_SUBSCRIPTION_ID is set"
            fi
            
            if [ -z "${{ secrets.ARM_TENANT_ID }}" ]; then
              echo "âŒ Missing: ARM_TENANT_ID"
              MISSING_SECRETS=1
            else
              echo "âœ… ARM_TENANT_ID is set"
            fi
          fi
          
          if [ $MISSING_SECRETS -eq 1 ]; then
            echo ""
            echo "âŒâŒâŒ VALIDATION FAILED âŒâŒâŒ"
            echo ""
            echo "Missing required secrets. Please configure them in:"
            echo "Repository Settings â†’ Secrets and variables â†’ Actions"
            echo ""
            echo "See README.md for detailed setup instructions."
            exit 1
          fi
          
          echo ""
          echo "âœ… All required secrets are configured!"

      - name: Start Provisioning Timer
        id: timer_start
        run: echo "start=$(date +%s)" >> "$GITHUB_OUTPUT"

      # AWS creds are required for S3 State Backend, even if deploying to Azure/GCP
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # GCP Authentication (Required for Terraform Provider init)
      - name: Configure GCP Credentials
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Extract GCP Project ID from Service Account Key
        if: ${{ contains(inputs.target_cloud, 'gcp') }}
        id: gcp_project
        run: |
          # Install jq if not available
          if ! command -v jq &> /dev/null; then
            sudo apt-get update -y && sudo apt-get install -y jq
          fi
          
          GCP_PROJECT_ID=$(echo '${{ secrets.GCP_SA_KEY }}' | jq -r '.project_id // empty' 2>/dev/null || echo "")
          if [[ -n "$GCP_PROJECT_ID" ]]; then
            echo "project_id=$GCP_PROJECT_ID" >> "$GITHUB_OUTPUT"
            echo "âœ… Extracted GCP Project ID: $GCP_PROJECT_ID"
          else
            echo "âš ï¸  Could not extract project_id from GCP_SA_KEY"
            echo "   Make sure the service account key JSON contains 'project_id' field"
            exit 1
          fi

      - name: Setup Google Cloud SDK
        if: ${{ contains(inputs.target_cloud, 'gcp') }}
        uses: google-github-actions/setup-gcloud@v2

      # Azure CLI Authentication (Required for az commands in null_resource)
      - name: Authenticate Azure CLI
        if: ${{ contains(inputs.target_cloud, 'azure') }}
        uses: azure/login@v2
        with:
          creds: |
            {
              "clientId": "${{ secrets.ARM_CLIENT_ID }}",
              "clientSecret": "${{ secrets.ARM_CLIENT_SECRET }}",
              "subscriptionId": "${{ secrets.ARM_SUBSCRIPTION_ID }}",
              "tenantId": "${{ secrets.ARM_TENANT_ID }}"
            }

      # Ensure awscli is available for S3 sync in the null_resource
      - name: Check AWS CLI
        if: ${{ inputs.target_cloud == 'aws' && inputs.deployment_mode == 'static' }}
        run: |
          if ! command -v aws >/dev/null 2>&1; then
            sudo apt-get update -y
            sudo apt-get install -y awscli
          fi
          aws --version

      # Check Docker for container mode with AWS/Azure (required for image mirroring)
      - name: Check Docker
        if: ${{ inputs.action == 'deploy' && inputs.deployment_mode == 'container' && (contains(inputs.target_cloud, 'aws') || contains(inputs.target_cloud, 'azure')) }}
        run: |
          echo "Checking Docker installation..."
          if ! command -v docker &> /dev/null; then
            echo "âŒ Docker is not installed"
            echo "Installing Docker..."
            sudo apt-get update -y
            sudo apt-get install -y docker.io
            sudo systemctl start docker
            sudo systemctl enable docker
            # Add runner user to docker group (GitHub Actions)
            sudo usermod -aG docker $USER || sudo usermod -aG docker runner || true
          fi
          
          # Check if Docker daemon is running
          if ! docker ps &> /dev/null; then
            echo "âš ï¸  Docker daemon is not running. Starting..."
            sudo systemctl start docker || sudo service docker start
            sleep 2
          fi
          
          docker --version
          docker ps
          echo "âœ… Docker is installed and running"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Get AWS Account ID
        id: get_account
        run: echo "account_id=$(aws sts get-caller-identity --query Account --output text)" >> "$GITHUB_OUTPUT"

      - name: Bootstrap Backend Infrastructure (S3 Bucket & DynamoDB Table)
        run: |
          BUCKET_NAME="multicloud-tf-state-${{ steps.get_account.outputs.account_id }}"
          TABLE_NAME="terraform-locks"
          REGION="${{ env.AWS_REGION }}"
          
          echo "Bootstrapping backend infrastructure..."
          
          # Create S3 Bucket if missing
          if ! aws s3api head-bucket --bucket "$BUCKET_NAME" 2>/dev/null; then
            echo "Creating State Bucket: $BUCKET_NAME..."
            aws s3api create-bucket \
              --bucket "$BUCKET_NAME" \
              --region "$REGION" \
              --create-bucket-configuration LocationConstraint="$REGION" >/dev/null
            aws s3api put-bucket-versioning \
              --bucket "$BUCKET_NAME" \
              --versioning-configuration Status=Enabled
            echo "âœ… S3 bucket created and versioning enabled"
          else
            echo "âœ… State Bucket $BUCKET_NAME already exists"
          fi
          
          # Create DynamoDB Table if missing
          if ! aws dynamodb describe-table --table-name "$TABLE_NAME" --region "$REGION" >/dev/null 2>&1; then
            echo "Creating Lock Table: $TABLE_NAME..."
            aws dynamodb create-table \
              --table-name "$TABLE_NAME" \
              --attribute-definitions AttributeName=LockID,AttributeType=S \
              --key-schema AttributeName=LockID,KeyType=HASH \
              --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1 \
              --region "$REGION" >/dev/null
            echo "âœ… DynamoDB table creation initiated (waiting for it to be active...)"
            aws dynamodb wait table-exists --table-name "$TABLE_NAME" --region "$REGION"
            echo "âœ… DynamoDB table is now active"
          else
            echo "âœ… Lock Table $TABLE_NAME already exists"
          fi

      - name: Terraform Init
        run: |
          terraform init -input=false \
            -backend-config="bucket=multicloud-tf-state-${{ steps.get_account.outputs.account_id }}" \
            -backend-config="key=global/s3/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="dynamodb_table=terraform-locks" \
            -backend-config="encrypt=true"

      - name: Terraform Plan
        if: ${{ inputs.action == 'deploy' }}
        id: plan
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
        run: |
          # Convert comma-separated string to JSON list
          CLOUDS_INPUT="${{ inputs.target_cloud }}"
          TF_CLOUDS_LIST="[\"$(echo $CLOUDS_INPUT | sed 's/,/\",\"/g')\"]"
          
          TF_PLAN_ARGS=(
            -input=false
            -var="target_clouds=$TF_CLOUDS_LIST"
            -var="deployment_mode=${{ inputs.deployment_mode }}"
            -var="app_image=${{ inputs.app_image }}"
            -var="domain_name=${{ inputs.domain_name }}"
            -var="enable_nat_gateway=${{ env.ENABLE_NAT_GATEWAY }}"
          )
          
          # Add GCP project ID if GCP is in target clouds
          if [[ "$CLOUDS_INPUT" == *"gcp"* ]] && [[ -n "${{ steps.gcp_project.outputs.project_id }}" ]]; then
            TF_PLAN_ARGS+=(-var="gcp_project_id=${{ steps.gcp_project.outputs.project_id }}")
          fi
          
          terraform plan "${TF_PLAN_ARGS[@]}" -out=tfplan.out

      - name: Terraform Apply (Provisioning)
        if: ${{ inputs.action == 'deploy' }}
        id: apply
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
        run: terraform apply -input=false tfplan.out

      - name: Export Terraform outputs safely
        if: ${{ inputs.action == 'deploy' }}
        id: tf_outputs
        shell: bash
        run: |
          set -euo pipefail
          CLOUD="${{ inputs.target_cloud }}"
          MODE="${{ inputs.deployment_mode }}"

          # Get all outputs as JSON; if none, keep it empty JSON
          OUT_JSON="$(terraform output -json || echo '{}')"

          # Helper to read a key if present
          jget() { echo "$OUT_JSON" | jq -r "$1 // empty"; }

          # AWS + k8s: guarded EKS outputs
          # Check if "aws" is in the target_cloud string (handles "aws", "aws,gcp", "aws,azure", "aws,gcp,azure", etc.)
          if [[ "$CLOUD" == *"aws"* ]] && [ "$MODE" = "k8s" ]; then
            EKS_CLUSTER_ENDPOINT="$(jget '.eks_cluster_endpoint.value')"
            EKS_CLUSTER_NAME="$(jget '.eks_cluster_name.value')"
            EKS_VPC_ID="$(jget '.eks_vpc_id.value')"
            EKS_LB_ROLE_ARN="$(jget '.eks_lb_role_arn.value')"
            ACM_CERT_ARN="$(jget '.eks_acm_certificate_arn.value')"
            
            if [ -n "$EKS_CLUSTER_ENDPOINT" ]; then
              echo "EKS_CLUSTER_ENDPOINT=$EKS_CLUSTER_ENDPOINT" >> "$GITHUB_ENV"
              echo "cluster_endpoint=$EKS_CLUSTER_ENDPOINT" >> "$GITHUB_OUTPUT"
            fi
            if [ -n "$EKS_CLUSTER_NAME" ]; then echo "EKS_CLUSTER_NAME=$EKS_CLUSTER_NAME" >> "$GITHUB_ENV"; fi
            if [ -n "$EKS_VPC_ID" ]; then echo "EKS_VPC_ID=$EKS_VPC_ID" >> "$GITHUB_ENV"; fi
            if [ -n "$EKS_LB_ROLE_ARN" ]; then echo "EKS_LB_ROLE_ARN=$EKS_LB_ROLE_ARN" >> "$GITHUB_ENV"; fi
            if [ -n "$ACM_CERT_ARN" ]; then echo "ACM_CERT_ARN=$ACM_CERT_ARN" >> "$GITHUB_ENV"; fi
          fi

          # GCP + k8s: guarded GKE outputs
          # Check if "gcp" is in the target_cloud string (handles multi-cloud scenarios)
          if [[ "$CLOUD" == *"gcp"* ]] && [ "$MODE" = "k8s" ]; then
            GKE_NAME="$(jget '.gke_cluster_name.value')"
            GKE_ENDPOINT="$(jget '.gke_cluster_endpoint.value')"
            echo "GKE_NAME=$GKE_NAME" >> "$GITHUB_ENV"
            if [ -n "$GKE_ENDPOINT" ]; then
               echo "GKE_CLUSTER_ENDPOINT=$GKE_ENDPOINT" >> "$GITHUB_ENV"
               echo "CLUSTER_ENDPOINT=$GKE_ENDPOINT" >> "$GITHUB_ENV"
               echo "cluster_endpoint=$GKE_ENDPOINT" >> "$GITHUB_OUTPUT"
            fi
          fi

          # Static mode: Extract URLs for each cloud separately
          if [ "$MODE" = "static" ]; then
            if [[ "$CLOUD" == *"aws"* ]]; then
              AWS_STATIC_URL="$(jget '.aws_static_cloudfront_url.value // empty')"
              if [ -n "$AWS_STATIC_URL" ] && [ "$AWS_STATIC_URL" != "null" ]; then
                echo "AWS_STATIC_URL=$AWS_STATIC_URL" >> "$GITHUB_ENV"
              fi
            fi
            if [[ "$CLOUD" == *"gcp"* ]]; then
              GCP_STATIC_URL="$(jget '.gcp_static_website_url.value // empty')"
              if [ -n "$GCP_STATIC_URL" ] && [ "$GCP_STATIC_URL" != "null" ]; then
                echo "GCP_STATIC_URL=$GCP_STATIC_URL" >> "$GITHUB_ENV"
              fi
            fi
            if [[ "$CLOUD" == *"azure"* ]]; then
              AZURE_STATIC_URL="$(jget '.azure_static_website_url.value // empty')"
              if [ -n "$AZURE_STATIC_URL" ] && [ "$AZURE_STATIC_URL" != "null" ]; then
                echo "AZURE_STATIC_URL=$AZURE_STATIC_URL" >> "$GITHUB_ENV"
              fi
            fi
          fi

          # Container mode: Extract URLs for each cloud separately
          if [ "$MODE" = "container" ]; then
            if [[ "$CLOUD" == *"aws"* ]]; then
              AWS_CONTAINER_URL="$(jget '.aws_container_service_url.value // empty')"
              if [ -n "$AWS_CONTAINER_URL" ] && [ "$AWS_CONTAINER_URL" != "null" ]; then
                echo "AWS_CONTAINER_URL=$AWS_CONTAINER_URL" >> "$GITHUB_ENV"
              fi
            fi
            if [[ "$CLOUD" == *"gcp"* ]]; then
              GCP_CONTAINER_URL="$(jget '.gcp_container_service_url.value // empty')"
              if [ -n "$GCP_CONTAINER_URL" ] && [ "$GCP_CONTAINER_URL" != "null" ]; then
                echo "GCP_CONTAINER_URL=$GCP_CONTAINER_URL" >> "$GITHUB_ENV"
              fi
            fi
            if [[ "$CLOUD" == *"azure"* ]]; then
              AZURE_CONTAINER_URL="$(jget '.azure_container_service_url.value // empty')"
              if [ -n "$AZURE_CONTAINER_URL" ] && [ "$AZURE_CONTAINER_URL" != "null" ]; then
                echo "AZURE_CONTAINER_URL=$AZURE_CONTAINER_URL" >> "$GITHUB_ENV"
              fi
            fi
          fi

      # ---- Ansible setup (must be before running Ansible) ----
      - name: Setup Python and Ansible
        if: ${{ inputs.action == 'deploy' && inputs.deployment_mode == 'k8s' }}
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install kubectl
        if: ${{ inputs.action == 'deploy' && inputs.deployment_mode == 'k8s' }}
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Install Helm
        if: ${{ inputs.action == 'deploy' && inputs.deployment_mode == 'k8s' }}
        uses: azure/setup-helm@v3
        with:
          version: 'latest'

      - name: Install Ansible + k8s deps
        if: ${{ inputs.action == 'deploy' && inputs.deployment_mode == 'k8s' }}
        run: |
          pip install ansible kubernetes openshift jmespath
          ansible-galaxy collection install kubernetes.core community.general

      # Configure kubeconfig for each cloud and run Ansible separately
      # This ensures ALB/domain is only configured for AWS, not GCP/Azure
      
      - name: Configure Kubeconfig for EKS (AWS)
        if: ${{ inputs.action == 'deploy' && contains(inputs.target_cloud, 'aws') && inputs.deployment_mode == 'k8s' }}
        run: |
          echo "Configuring kubeconfig for EKS (AWS)..."
          if [[ -n "${{ env.EKS_CLUSTER_NAME }}" ]]; then
            aws eks update-kubeconfig --region "${{ env.AWS_REGION }}" --name "${{ env.EKS_CLUSTER_NAME }}" --kubeconfig ./kubeconfig.yaml
            chmod 600 ./kubeconfig.yaml
            echo "âœ… EKS kubeconfig configured"
          else
            echo "âš ï¸  EKS cluster name not found in outputs"
          fi

      - name: Run Ansible for EKS (AWS with ALB/Domain)
        if: ${{ inputs.action == 'deploy' && contains(inputs.target_cloud, 'aws') && inputs.deployment_mode == 'k8s' }}
        env:
          KUBECONFIG: ${{ github.workspace }}/kubeconfig.yaml
        run: |
          if [ ! -f ansible/inventory.yml ] || [ ! -f ansible/playbook.yml ]; then
            echo "Ansible files not present, skipping."
            exit 0
          fi
          
          # Verify kubeconfig exists
          if [ ! -f "${{ github.workspace }}/kubeconfig.yaml" ]; then
            echo "âŒ Error: kubeconfig.yaml not found at ${{ github.workspace }}/kubeconfig.yaml"
            exit 1
          fi
          
          # For AWS, include domain and AWS-specific vars (ALB will be installed)
          EXTRA_VARS="deployment_mode=${{ inputs.deployment_mode }} app_image=${{ inputs.app_image }} domain_name=${{ inputs.domain_name }} target_clouds=aws acm_certificate_arn=${{ env.ACM_CERT_ARN }} eks_cluster_name=${{ env.EKS_CLUSTER_NAME }} eks_vpc_id=${{ env.EKS_VPC_ID }} aws_region=${{ env.AWS_REGION }} eks_lb_role_arn=${{ env.EKS_LB_ROLE_ARN }}"
          
          echo "Running Ansible for EKS (AWS) with ALB/Domain support..."
          ansible-playbook -i ansible/inventory.yml ansible/playbook.yml --extra-vars "$EXTRA_VARS"

      - name: Configure Kubeconfig for GKE (GCP)
        if: ${{ inputs.action == 'deploy' && contains(inputs.target_cloud, 'gcp') && inputs.deployment_mode == 'k8s' }}
        run: |
          echo "Configuring kubeconfig for GKE (GCP)..."
          if [[ -n "${{ env.GKE_NAME }}" ]]; then
            # Ensure gcloud is authenticated with service account
            echo "Verifying gcloud authentication..."
            gcloud auth list
            
            # Get the project ID
            GCP_PROJECT=$(gcloud config get-value project 2>/dev/null || echo "")
            if [ -z "$GCP_PROJECT" ]; then
              echo "âš ï¸  Warning: Could not get GCP project. Trying to extract from service account..."
              GCP_PROJECT=$(echo '${{ secrets.GCP_SA_KEY }}' | jq -r '.project_id // empty' 2>/dev/null || echo "")
            fi
            
            if [ -z "$GCP_PROJECT" ]; then
              echo "âŒ Error: Could not determine GCP project ID"
              exit 1
            fi
            
            echo "Using GCP Project: $GCP_PROJECT"
            
            # Install gke-gcloud-auth-plugin if not present (required for GKE kubectl authentication)
            if ! command -v gke-gcloud-auth-plugin &> /dev/null; then
              echo "Installing gke-gcloud-auth-plugin..."
              gcloud components install gke-gcloud-auth-plugin --quiet || true
            fi
            
            # Get credentials and write directly to kubeconfig.yaml using absolute path
            KUBECONFIG_PATH="${{ github.workspace }}/kubeconfig.yaml"
            
            # If kubeconfig exists and contains EKS entries (multi-cloud deployment), 
            # use a separate file for GKE to avoid conflicts
            if [ -f "$KUBECONFIG_PATH" ] && grep -q "eks.amazonaws.com" "$KUBECONFIG_PATH" 2>/dev/null; then
              echo "âš ï¸  Kubeconfig exists with EKS entries. Using separate file for GKE..."
              GKE_KUBECONFIG="${{ github.workspace }}/kubeconfig-gke.yaml"
              
              # Fetch GKE credentials to separate file
              export KUBECONFIG="$GKE_KUBECONFIG"
              echo "Fetching GKE credentials to separate file..."
              gcloud container clusters get-credentials "${{ env.GKE_NAME }}" \
                --region "us-central1" \
                --project "$GCP_PROJECT"
              
              # Use both kubeconfigs via KUBECONFIG env var (kubectl will merge them automatically)
              export KUBECONFIG="$KUBECONFIG_PATH:$GKE_KUBECONFIG"
              echo "âœ… GKE kubeconfig configured (using multi-file kubeconfig)"
            else
              # Single cloud or no existing kubeconfig - use directly
              export KUBECONFIG="$KUBECONFIG_PATH"
              
              echo "Fetching GKE credentials..."
              # gcloud doesn't support --kubeconfig flag, so we set KUBECONFIG env var
              gcloud container clusters get-credentials "${{ env.GKE_NAME }}" \
                --region "us-central1" \
                --project "$GCP_PROJECT"
            fi
            
            # Verify kubeconfig is valid and has proper authentication
            if [ -f "$KUBECONFIG_PATH" ]; then
              chmod 600 "$KUBECONFIG_PATH"
            fi
            if [ -n "${GKE_KUBECONFIG:-}" ] && [ -f "$GKE_KUBECONFIG" ]; then
              chmod 600 "$GKE_KUBECONFIG"
            fi
            
            echo "Verifying kubeconfig authentication..."
            
            # Test cluster access (use KUBECONFIG env var which handles multi-file automatically)
            if kubectl cluster-info &>/dev/null; then
              echo "âœ… GKE kubeconfig configured and authenticated"
              # Try to get GKE nodes specifically
              kubectl get nodes --context="gke_${GCP_PROJECT}_us-central1_${{ env.GKE_NAME }}" 2>/dev/null || \
              kubectl get nodes 2>/dev/null || echo "âš ï¸  Warning: Could not list nodes (cluster may still be provisioning)"
            else
              echo "âŒ Error: Failed to authenticate with GKE cluster"
              echo "Checking kubeconfig contents..."
              if [ -n "${GKE_KUBECONFIG:-}" ] && [ -f "$GKE_KUBECONFIG" ]; then
                cat "$GKE_KUBECONFIG" | head -30
              else
                cat "$KUBECONFIG_PATH" | head -30
              fi
              exit 1
            fi
          else
            echo "âš ï¸  GKE cluster name not found in outputs"
          fi

      - name: Run Ansible for GKE (GCP - LoadBalancer only, no domain)
        if: ${{ inputs.action == 'deploy' && contains(inputs.target_cloud, 'gcp') && inputs.deployment_mode == 'k8s' }}
        run: |
          if [ ! -f ansible/inventory.yml ] || [ ! -f ansible/playbook.yml ]; then
            echo "Ansible files not present, skipping."
            exit 0
          fi
          
          KUBECONFIG_PATH="${{ github.workspace }}/kubeconfig.yaml"
          GKE_KUBECONFIG="${{ github.workspace }}/kubeconfig-gke.yaml"
          
          # Set KUBECONFIG to include both files if GKE kubeconfig exists (multi-cloud)
          if [ -f "$GKE_KUBECONFIG" ]; then
            export KUBECONFIG="$KUBECONFIG_PATH:$GKE_KUBECONFIG"
            echo "Using multi-file kubeconfig: $KUBECONFIG"
          else
            export KUBECONFIG="$KUBECONFIG_PATH"
          fi
          
          # Verify kubeconfig exists and is accessible
          if [ ! -f "$KUBECONFIG_PATH" ]; then
            echo "âŒ Error: kubeconfig.yaml not found at $KUBECONFIG_PATH!"
            exit 1
          fi
          
          # Test kubectl access before running Ansible
          echo "Testing kubectl access to GKE cluster..."
          if ! kubectl cluster-info &>/dev/null; then
            echo "âŒ Error: Cannot access GKE cluster. Authentication failed."
            echo "Current gcloud auth:"
            gcloud auth list
            echo "Kubeconfig: $KUBECONFIG"
            exit 1
          fi
          
          echo "âœ… kubectl access verified"
          
          # For GCP, no domain, no AWS vars - just LoadBalancer service (no ALB, no Ingress)
          EXTRA_VARS="deployment_mode=${{ inputs.deployment_mode }} app_image=${{ inputs.app_image }} domain_name= target_clouds=gcp"
          
          echo "Running Ansible for GKE (GCP) with LoadBalancer service (no domain, no ALB)..."
          ansible-playbook -i ansible/inventory.yml ansible/playbook.yml --extra-vars "$EXTRA_VARS"

      - name: Configure Kubeconfig for AKS (Azure)
        if: ${{ inputs.action == 'deploy' && contains(inputs.target_cloud, 'azure') && inputs.deployment_mode == 'k8s' }}
        run: |
          echo "Configuring kubeconfig for AKS (Azure)..."
          AKS_NAME=$(terraform output -raw aks_cluster_name 2>/dev/null || echo "")
          AKS_RG=$(terraform output -raw aks_resource_group_name 2>/dev/null || echo "")
          
          if [[ -n "$AKS_NAME" ]] && [[ -n "$AKS_RG" ]]; then
            KUBECONFIG_PATH="${{ github.workspace }}/kubeconfig.yaml"
            
            # If kubeconfig exists and contains other cloud entries (multi-cloud deployment), 
            # use a separate file for AKS to avoid conflicts
            if [ -f "$KUBECONFIG_PATH" ] && (grep -q "eks.amazonaws.com" "$KUBECONFIG_PATH" 2>/dev/null || grep -q "gke" "$KUBECONFIG_PATH" 2>/dev/null); then
              echo "âš ï¸  Kubeconfig exists with other cloud entries. Using separate file for AKS..."
              AKS_KUBECONFIG="${{ github.workspace }}/kubeconfig-aks.yaml"
              
              # Fetch AKS credentials to separate file
              az aks get-credentials --resource-group "$AKS_RG" --name "$AKS_NAME" --file "$AKS_KUBECONFIG" --overwrite-existing
              chmod 600 "$AKS_KUBECONFIG"
              
              # Use both kubeconfigs via KUBECONFIG env var (kubectl will merge them automatically)
              export KUBECONFIG="$KUBECONFIG_PATH:$AKS_KUBECONFIG"
              echo "âœ… AKS kubeconfig configured (using multi-file kubeconfig)"
            else
              # Single cloud or no existing kubeconfig - use directly
              az aks get-credentials --resource-group "$AKS_RG" --name "$AKS_NAME" --file "$KUBECONFIG_PATH" --overwrite-existing
              chmod 600 "$KUBECONFIG_PATH"
              export KUBECONFIG="$KUBECONFIG_PATH"
              echo "âœ… AKS kubeconfig configured"
            fi
            
            # Verify kubeconfig is valid and has proper authentication
            if [ -f "$KUBECONFIG_PATH" ]; then
              chmod 600 "$KUBECONFIG_PATH"
            fi
            if [ -n "${AKS_KUBECONFIG:-}" ] && [ -f "$AKS_KUBECONFIG" ]; then
              chmod 600 "$AKS_KUBECONFIG"
            fi
            
            echo "Verifying kubeconfig authentication..."
            
            # Test cluster access (use KUBECONFIG env var which handles multi-file automatically)
            if kubectl cluster-info &>/dev/null; then
              echo "âœ… AKS kubeconfig configured and authenticated"
              # Try to get AKS nodes specifically
              kubectl get nodes --context="$(kubectl config get-contexts -o name | grep -i aks | head -1)" 2>/dev/null || \
              kubectl get nodes 2>/dev/null || echo "âš ï¸  Warning: Could not list nodes (cluster may still be provisioning)"
            else
              echo "âŒ Error: Failed to authenticate with AKS cluster"
              echo "Checking kubeconfig contents..."
              if [ -n "${AKS_KUBECONFIG:-}" ] && [ -f "$AKS_KUBECONFIG" ]; then
                cat "$AKS_KUBECONFIG" | head -30
              else
                cat "$KUBECONFIG_PATH" | head -30
              fi
              exit 1
            fi
          else
            echo "âš ï¸  AKS cluster name or resource group not found in outputs"
          fi

      - name: Run Ansible for AKS (Azure - LoadBalancer only, no domain)
        if: ${{ inputs.action == 'deploy' && contains(inputs.target_cloud, 'azure') && inputs.deployment_mode == 'k8s' }}
        run: |
          if [ ! -f ansible/inventory.yml ] || [ ! -f ansible/playbook.yml ]; then
            echo "Ansible files not present, skipping."
            exit 0
          fi
          
          KUBECONFIG_PATH="${{ github.workspace }}/kubeconfig.yaml"
          AKS_KUBECONFIG="${{ github.workspace }}/kubeconfig-aks.yaml"
          GKE_KUBECONFIG="${{ github.workspace }}/kubeconfig-gke.yaml"
          
          # Set KUBECONFIG to include all relevant files if they exist (multi-cloud)
          KUBECONFIG_VAR="$KUBECONFIG_PATH"
          if [ -f "$GKE_KUBECONFIG" ]; then
            KUBECONFIG_VAR="$KUBECONFIG_VAR:$GKE_KUBECONFIG"
          fi
          if [ -f "$AKS_KUBECONFIG" ]; then
            KUBECONFIG_VAR="$KUBECONFIG_VAR:$AKS_KUBECONFIG"
          fi
          export KUBECONFIG="$KUBECONFIG_VAR"
          
          # Verify kubeconfig exists and is accessible
          if [ ! -f "$KUBECONFIG_PATH" ] && [ ! -f "$AKS_KUBECONFIG" ]; then
            echo "âŒ Error: kubeconfig files not found!"
            exit 1
          fi
          
          # Test kubectl access before running Ansible
          echo "Testing kubectl access to AKS cluster..."
          if ! kubectl cluster-info &>/dev/null; then
            echo "âŒ Error: Cannot access AKS cluster. Authentication failed."
            echo "Current az account:"
            az account show || true
            echo "Kubeconfig: $KUBECONFIG"
            exit 1
          fi
          
          echo "âœ… kubectl access verified"
          
          # For Azure, no domain, no AWS vars - just LoadBalancer service (no ALB, no Ingress)
          EXTRA_VARS="deployment_mode=${{ inputs.deployment_mode }} app_image=${{ inputs.app_image }} domain_name= target_clouds=azure"
          
          echo "Running Ansible for AKS (Azure) with LoadBalancer service (no domain, no ALB)..."
          ansible-playbook -i ansible/inventory.yml ansible/playbook.yml --extra-vars "$EXTRA_VARS"

      - name: Measure Provisioning Time
        if: ${{ inputs.action == 'deploy' }}
        id: time_provisioning
        run: |
          end=$(date +%s)
          start=${{ steps.timer_start.outputs.start }}
          echo "duration=$((end - start))" >> "$GITHUB_OUTPUT"
          echo "Provisioning Time: $((end - start)) seconds"

      - name: Update Route 53 DNS (Auto - AWS only)
        if: ${{ inputs.action == 'deploy' && inputs.deployment_mode == 'k8s' && contains(inputs.target_cloud, 'aws') && inputs.domain_name != '' }}
        env:
          KUBECONFIG: ${{ github.workspace }}/kubeconfig.yaml
          DOMAIN_NAME: ${{ inputs.domain_name }}
          AWS_REGION: ${{ env.AWS_REGION }}
        run: |
          echo "Waiting for Ingress to be assigned a hostname..."
          ALB_HOSTNAME=""
          for i in {1..24}; do
            ALB_HOSTNAME=$(kubectl get ingress -n sample-app portfolio-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            if [[ -n "$ALB_HOSTNAME" ]]; then
              echo "Found ALB Hostname: $ALB_HOSTNAME"
              break
            fi
            sleep 5
          done
          
          if [[ -n "$ALB_HOSTNAME" ]]; then
            # Get Hosted Zone ID (assume we can query it or get from terraform)
            # We can get it from terraform output but we didn't export it to ENV. Let's export it in the 'Export Terraform outputs safely' step first?
            # Or just query AWS since we have creds.
            
            HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?Name=='$DOMAIN_NAME.'].Id" --output text | cut -d'/' -f3)
            
            if [[ -n "$HOSTED_ZONE_ID" ]]; then
               # Get ALB Zone ID
               ALB_ZONE_ID=$(aws elbv2 describe-load-balancers --region "$AWS_REGION" --query "LoadBalancers[?DNSName=='$ALB_HOSTNAME'].CanonicalHostedZoneId" --output text)
               
               if [[ -n "$ALB_ZONE_ID" ]]; then
                 echo "Updating Route 53..."
                 cat > change-batch.json <<EOF
          {
            "Comment": "Auto-update Alias record for ALB",
            "Changes": [
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "$DOMAIN_NAME",
                  "Type": "A",
                  "AliasTarget": {
                    "HostedZoneId": "$ALB_ZONE_ID",
                    "DNSName": "$ALB_HOSTNAME",
                    "EvaluateTargetHealth": true
                  }
                }
              },
              {
                "Action": "UPSERT",
                "ResourceRecordSet": {
                  "Name": "www.$DOMAIN_NAME",
                  "Type": "A",
                  "AliasTarget": {
                    "HostedZoneId": "$ALB_ZONE_ID",
                    "DNSName": "$ALB_HOSTNAME",
                    "EvaluateTargetHealth": true
                  }
                }
              }
            ]
          }
          EOF
                 aws route53 change-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" --change-batch file://change-batch.json
                 echo "DNS Updated."
               fi
            fi
          fi

      # Display Terraform Outputs
      - name: Display Terraform Outputs
        if: ${{ inputs.action == 'deploy' && success() }}
        run: |
          echo "=========================================="
          echo "ðŸ“Š Deployment Outputs"
          echo "=========================================="
          if terraform output >/dev/null 2>&1; then
            terraform output
          else
            echo "âš ï¸  No outputs available"
          fi

      # Display Nameserver Instructions (if domain provided)
      - name: Display Nameserver Instructions
        if: ${{ inputs.action == 'deploy' && inputs.domain_name != '' && contains(inputs.target_cloud, 'aws') && success() }}
        run: |
          echo ""
          echo "=========================================="
          echo "âš ï¸  IMPORTANT: Update Domain Nameservers"
          echo "=========================================="
          
          # Extract apex domain
          DOMAIN="${{ inputs.domain_name }}"
          if [[ "$DOMAIN" =~ ^www\. ]]; then
            APEX_DOMAIN="${DOMAIN#www.}"
          elif [[ "$DOMAIN" =~ ^[^.]+\. ]]; then
            PARTS=$(echo "$DOMAIN" | tr '.' '\n' | wc -l)
            if [[ $PARTS -gt 2 ]]; then
              APEX_DOMAIN=$(echo "$DOMAIN" | sed 's/^[^.]*\.//')
            else
              APEX_DOMAIN="$DOMAIN"
            fi
          else
            APEX_DOMAIN="$DOMAIN"
          fi
          
          # Get nameservers from Route 53
          ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name "$APEX_DOMAIN" --query 'HostedZones[0].Id' --output text 2>/dev/null | cut -d'/' -f3)
          
          if [[ -n "$ZONE_ID" ]] && [[ "$ZONE_ID" != "None" ]]; then
            NAMESERVERS=$(aws route53 get-hosted-zone --id "$ZONE_ID" --query 'DelegationSet.NameServers' --output text 2>/dev/null | tr '\t' '\n')
            
            if [[ -n "$NAMESERVERS" ]]; then
              echo "Route 53 Nameservers for $APEX_DOMAIN:"
              echo "$NAMESERVERS" | sed 's/^/  - /'
              echo ""
              echo "ACTION REQUIRED:"
              echo "1. Go to your domain registrar (where you bought $APEX_DOMAIN)"
              echo "2. Update nameservers to the values above"
              echo "3. Wait 10-30 minutes for DNS propagation"
              echo "4. Certificate will auto-validate once DNS propagates"
            else
              echo "âš ï¸  Could not retrieve nameservers"
            fi
          else
            echo "âš ï¸  Hosted zone not found for $APEX_DOMAIN"
          fi

      # Display all deployment URLs
      - name: Display Deployment URLs
        if: ${{ inputs.action == 'deploy' && success() }}
        env:
          KUBECONFIG: ${{ github.workspace }}/kubeconfig.yaml
        run: |
          echo ""
          echo "=========================================="
          echo "ðŸŒ Deployment URLs"
          echo "=========================================="
          echo "Cloud: ${{ inputs.target_cloud }}"
          echo "Mode:  ${{ inputs.deployment_mode }}"
          echo ""
          
          # K8s Mode URLs
          if [ "${{ inputs.deployment_mode }}" = "k8s" ]; then
            echo "ðŸ“¦ Kubernetes Clusters:"
            if [[ "${{ inputs.target_cloud }}" == *"aws"* ]] && [ -n "${{ env.EKS_CLUSTER_ENDPOINT }}" ]; then
              echo "  - AWS EKS API: ${{ env.EKS_CLUSTER_ENDPOINT }}"
              if [[ -n "${{ inputs.domain_name }}" ]]; then
                echo "  - AWS Application: https://${{ inputs.domain_name }} (ALB with custom domain)"
              else
                # Try to get LoadBalancer URL from kubectl
                if [ -f kubeconfig.yaml ]; then
                  export KUBECONFIG=${{ github.workspace }}/kubeconfig.yaml
                  LB_URL=$(kubectl get svc -n sample-app portfolio-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || kubectl get svc -n sample-app portfolio-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
                  if [ -n "$LB_URL" ]; then
                    echo "  - AWS Application: http://$LB_URL (LoadBalancer)"
                  fi
                fi
              fi
            fi
            if [[ "${{ inputs.target_cloud }}" == *"gcp"* ]] && [ -n "${{ env.GKE_CLUSTER_ENDPOINT }}" ]; then
              echo "  - GCP GKE API: ${{ env.GKE_CLUSTER_ENDPOINT }}"
              # Try to get LoadBalancer URL from kubectl
              if [ -f kubeconfig.yaml ]; then
                export KUBECONFIG=${{ github.workspace }}/kubeconfig.yaml
                LB_URL=$(kubectl get svc -n sample-app portfolio-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
                if [ -n "$LB_URL" ]; then
                  echo "  - GCP Application: http://$LB_URL (LoadBalancer)"
                fi
              fi
            fi
            if [[ "${{ inputs.target_cloud }}" == *"azure"* ]]; then
              # Get AKS endpoint from terraform output
              AKS_ENDPOINT=$(terraform output -raw aks_cluster_endpoint 2>/dev/null || echo "")
              if [ -n "$AKS_ENDPOINT" ]; then
                echo "  - Azure AKS API: $AKS_ENDPOINT"
              fi
              # Try to get LoadBalancer URL from kubectl
              if [ -f kubeconfig.yaml ]; then
                export KUBECONFIG=${{ github.workspace }}/kubeconfig.yaml
                LB_URL=$(kubectl get svc -n sample-app portfolio-service -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || echo "")
                if [ -n "$LB_URL" ]; then
                  echo "  - Azure AKS Application: http://$LB_URL (LoadBalancer)"
                fi
              fi
            fi
          fi
          
          # Container Mode URLs
          if [ "${{ inputs.deployment_mode }}" = "container" ]; then
            echo "ðŸ³ Container Services:"
            if [[ "${{ inputs.target_cloud }}" == *"aws"* ]] && [ -n "${{ env.AWS_CONTAINER_URL }}" ]; then
              echo "  - AWS App Runner: ${{ env.AWS_CONTAINER_URL }}"
              if [[ -n "${{ inputs.domain_name }}" ]]; then
                echo "    Custom Domain: https://aws.${{ inputs.domain_name }}"
              fi
            fi
            if [[ "${{ inputs.target_cloud }}" == *"gcp"* ]] && [ -n "${{ env.GCP_CONTAINER_URL }}" ]; then
              echo "  - GCP Cloud Run: ${{ env.GCP_CONTAINER_URL }}"
              if [[ -n "${{ inputs.domain_name }}" ]]; then
                echo "    Custom Domain: https://gcp.${{ inputs.domain_name }}"
              fi
            fi
            if [[ "${{ inputs.target_cloud }}" == *"azure"* ]] && [ -n "${{ env.AZURE_CONTAINER_URL }}" ]; then
              echo "  - Azure Container Instance: ${{ env.AZURE_CONTAINER_URL }}"
              if [[ -n "${{ inputs.domain_name }}" ]]; then
                echo "    Custom Domain: http://azure.${{ inputs.domain_name }}"
              fi
            fi
          fi
          
          # Static Mode URLs
          if [ "${{ inputs.deployment_mode }}" = "static" ]; then
            echo "ðŸ“„ Static Websites:"
            if [[ "${{ inputs.target_cloud }}" == *"aws"* ]] && [ -n "${{ env.AWS_STATIC_URL }}" ]; then
              echo "  - AWS CloudFront: ${{ env.AWS_STATIC_URL }}"
              if [[ -n "${{ inputs.domain_name }}" ]]; then
                echo "    Custom Domain: https://aws.${{ inputs.domain_name }}"
              fi
            fi
            if [[ "${{ inputs.target_cloud }}" == *"gcp"* ]] && [ -n "${{ env.GCP_STATIC_URL }}" ]; then
              echo "  - GCP Storage: ${{ env.GCP_STATIC_URL }}"
              if [[ -n "${{ inputs.domain_name }}" ]]; then
                echo "    Custom Domain: https://gcp.${{ inputs.domain_name }}"
              fi
            fi
            if [[ "${{ inputs.target_cloud }}" == *"azure"* ]] && [ -n "${{ env.AZURE_STATIC_URL }}" ]; then
              echo "  - Azure Storage: ${{ env.AZURE_STATIC_URL }}"
              if [[ -n "${{ inputs.domain_name }}" ]]; then
                echo "    Custom Domain: http://azure.${{ inputs.domain_name }}"
              fi
            fi
          fi
          
          echo ""
          
      # Verify cluster endpoints with cloud CLI commands
      - name: Verify Cluster Endpoints (Multi-Cloud)
        if: ${{ inputs.action == 'deploy' && inputs.deployment_mode == 'k8s' }}
        run: |
          echo ""
          echo "=========================================="
          echo "ðŸ” Verifying Cluster Endpoints"
          echo "=========================================="
          
          # Verify AWS EKS endpoint
          if [[ "${{ inputs.target_cloud }}" == *"aws"* ]] && [ -n "${{ env.EKS_CLUSTER_NAME }}" ]; then
            echo ""
            echo "AWS EKS:"
            echo "  Terraform Output: ${{ env.EKS_CLUSTER_ENDPOINT }}"
            AWS_ACTUAL=$(aws eks describe-cluster --name "${{ env.EKS_CLUSTER_NAME }}" --query 'cluster.endpoint' --output text 2>/dev/null || echo "N/A")
            echo "  AWS CLI Result: $AWS_ACTUAL"
            if [ "$AWS_ACTUAL" != "N/A" ] && [ "$AWS_ACTUAL" != "${{ env.EKS_CLUSTER_ENDPOINT }}" ]; then
              echo "  âš ï¸  WARNING: Endpoints don't match!"
            else
              echo "  âœ… Endpoints match"
            fi
          fi
          
          # Verify GCP GKE endpoint
          if [[ "${{ inputs.target_cloud }}" == *"gcp"* ]] && [ -n "${{ env.GKE_NAME }}" ]; then
            echo ""
            echo "GCP GKE:"
            echo "  Terraform Output: ${{ env.GKE_CLUSTER_ENDPOINT }}"
            GCP_PROJECT=$(gcloud config get-value project 2>/dev/null || echo "$(echo '${{ secrets.GCP_SA_KEY }}' | jq -r '.project_id // empty' 2>/dev/null || echo '')")
            if [ -n "$GCP_PROJECT" ]; then
              GCP_ACTUAL=$(gcloud container clusters describe "${{ env.GKE_NAME }}" --region "us-central1" --project "$GCP_PROJECT" --format="value(endpoint)" 2>/dev/null || echo "N/A")
              echo "  GCP CLI Result: $GCP_ACTUAL"
              if [ "$GCP_ACTUAL" != "N/A" ] && [ "$GCP_ACTUAL" != "${{ env.GKE_CLUSTER_ENDPOINT }}" ]; then
                echo "  âš ï¸  WARNING: Endpoints don't match!"
              else
                echo "  âœ… Endpoints match"
              fi
            else
              echo "  âš ï¸  Could not determine GCP project"
            fi
          fi
          
          # Verify Azure AKS endpoint
          if [[ "${{ inputs.target_cloud }}" == *"azure"* ]]; then
            echo ""
            echo "Azure AKS:"
            AKS_NAME=$(terraform output -raw aks_cluster_name 2>/dev/null || echo "")
            AKS_RG=$(terraform output -raw aks_resource_group_name 2>/dev/null || echo "")
            AKS_TF_ENDPOINT=$(terraform output -raw aks_cluster_endpoint 2>/dev/null || echo "N/A")
            echo "  Terraform Output: $AKS_TF_ENDPOINT"
            if [ -n "$AKS_NAME" ] && [ -n "$AKS_RG" ]; then
              AZURE_ACTUAL=$(az aks show --resource-group "$AKS_RG" --name "$AKS_NAME" --query 'fqdn' -o tsv 2>/dev/null || echo "N/A")
              # AKS endpoint is usually in format: <cluster-name>.<region>.cloudapp.azure.com
              # But the API endpoint might be different, let's get the kubeconfig host
              AKS_API_ENDPOINT=$(az aks show --resource-group "$AKS_RG" --name "$AKS_NAME" --query 'privateFqdn' -o tsv 2>/dev/null || az aks show --resource-group "$AKS_RG" --name "$AKS_NAME" --query 'fqdn' -o tsv 2>/dev/null || echo "N/A")
              echo "  Azure CLI FQDN: $AZURE_ACTUAL"
              echo "  Azure CLI API Endpoint: $AKS_API_ENDPOINT"
              # Extract IP from endpoint if it's an IP
              if [[ "$AKS_TF_ENDPOINT" =~ ^https?://([0-9]+\.[0-9]+\.[0-9]+\.[0-9]+) ]]; then
                AKS_TF_IP="${BASH_REMATCH[1]}"
                echo "  Terraform Endpoint IP: $AKS_TF_IP"
              fi
            else
              echo "  âš ï¸  Could not get AKS cluster name or resource group"
            fi
          fi
          
          echo ""
          echo "=========================================="

      # ==========================================
      # DESTROY WORKFLOW
      # ==========================================
      
      # Pre-destroy cleanup: Delete Kubernetes resources (Ingress and LoadBalancer Services)
      - name: Pre-Destroy Cleanup (Kubernetes Resources)
        if: ${{ inputs.action == 'destroy' && inputs.deployment_mode == 'k8s' }}
        run: |
          KUBECONFIG_PATH="${{ github.workspace }}/kubeconfig.yaml"
          GKE_KUBECONFIG="${{ github.workspace }}/kubeconfig-gke.yaml"
          AKS_KUBECONFIG="${{ github.workspace }}/kubeconfig-aks.yaml"
          
          # Set KUBECONFIG to include all relevant files if they exist (multi-cloud)
          KUBECONFIG_VAR="$KUBECONFIG_PATH"
          if [ -f "$GKE_KUBECONFIG" ]; then
            KUBECONFIG_VAR="$KUBECONFIG_VAR:$GKE_KUBECONFIG"
          fi
          if [ -f "$AKS_KUBECONFIG" ]; then
            KUBECONFIG_VAR="$KUBECONFIG_VAR:$AKS_KUBECONFIG"
          fi
          
          if [ -f "$KUBECONFIG_PATH" ] || [ -f "$GKE_KUBECONFIG" ] || [ -f "$AKS_KUBECONFIG" ]; then
            export KUBECONFIG="$KUBECONFIG_VAR"
            echo "Using multi-file kubeconfig for cleanup: $KUBECONFIG"
          else
            echo "âš ï¸  No kubeconfig found, skipping Kubernetes cleanup"
            exit 0
          fi
          
          # Delete Ingress (AWS ALB) - try all contexts
          echo "Attempting to delete Ingress (AWS ALB)..."
          for context in $(kubectl config get-contexts -o name 2>/dev/null || echo ""); do
            if kubectl --context="$context" get ingress -n sample-app portfolio-ingress &> /dev/null; then
              echo "Deleting Ingress from context: $context"
              kubectl --context="$context" delete ingress -n sample-app portfolio-ingress --ignore-not-found=true --timeout=60s || true
            fi
          done
          # Also try without context (default)
          if kubectl get ingress -n sample-app portfolio-ingress &> /dev/null 2>&1; then
            echo "Deleting Ingress (default context)..."
            kubectl delete ingress -n sample-app portfolio-ingress --ignore-not-found=true --timeout=60s || true
          fi
          echo "Waiting for ALB cleanup..."
          sleep 15
          
          # Delete LoadBalancer Service (GCP GKE and Azure AKS) - try all contexts
          echo "Attempting to delete LoadBalancer Service..."
          for context in $(kubectl config get-contexts -o name 2>/dev/null || echo ""); do
            if kubectl --context="$context" get svc -n sample-app portfolio-service &> /dev/null; then
              echo "Deleting LoadBalancer Service from context: $context"
              kubectl --context="$context" delete svc -n sample-app portfolio-service --ignore-not-found=true --timeout=60s || true
            fi
          done
          # Also try without context (default)
          if kubectl get svc -n sample-app portfolio-service &> /dev/null 2>&1; then
            echo "Deleting LoadBalancer Service (default context)..."
            kubectl delete svc -n sample-app portfolio-service --ignore-not-found=true --timeout=60s || true
          fi
          echo "Waiting for LoadBalancer cleanup..."
          sleep 20
          
          # Delete namespace and all resources within it - try all contexts
          echo "Attempting to delete namespace..."
          for context in $(kubectl config get-contexts -o name 2>/dev/null || echo ""); do
            if kubectl --context="$context" get namespace sample-app &> /dev/null; then
              echo "Deleting namespace from context: $context"
              kubectl --context="$context" delete namespace sample-app --ignore-not-found=true --timeout=120s || true
            fi
          done
          # Also try without context (default)
          if kubectl get namespace sample-app &> /dev/null 2>&1; then
            echo "Deleting namespace (default context)..."
            kubectl delete namespace sample-app --ignore-not-found=true --timeout=120s || true
          fi
          echo "Waiting for namespace cleanup..."
          sleep 10

      # Pre-destroy cleanup: Delete Route 53 records
      - name: Pre-Destroy Cleanup (Route 53 Records)
        if: ${{ inputs.action == 'destroy' && inputs.domain_name != '' && contains(inputs.target_cloud, 'aws') }}
        run: |
          echo "Cleaning up Route 53 records before hosted zone deletion..."
          
          # Extract apex domain
          DOMAIN="${{ inputs.domain_name }}"
          if [[ "$DOMAIN" =~ ^www\. ]]; then
            APEX_DOMAIN="${DOMAIN#www.}"
          elif [[ "$DOMAIN" =~ ^[^.]+\. ]]; then
            PARTS=$(echo "$DOMAIN" | tr '.' '\n' | wc -l)
            if [[ $PARTS -gt 2 ]]; then
              APEX_DOMAIN=$(echo "$DOMAIN" | sed 's/^[^.]*\.//')
            else
              APEX_DOMAIN="$DOMAIN"
            fi
          else
            APEX_DOMAIN="$DOMAIN"
          fi
          
          # Find hosted zone
          ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name "$APEX_DOMAIN" --query 'HostedZones[0].Id' --output text 2>/dev/null | cut -d'/' -f3)
          
          if [[ -n "$ZONE_ID" ]] && [[ "$ZONE_ID" != "None" ]]; then
            echo "Found hosted zone: $ZONE_ID"
            
            # Get all records except NS and SOA
            ALL_RECORDS=$(aws route53 list-resource-record-sets --hosted-zone-id "$ZONE_ID" \
              --max-items 1000 \
              --query "ResourceRecordSets[?Type != 'NS' && Type != 'SOA']" \
              --output json 2>/dev/null || echo "[]")
            
            if command -v jq &> /dev/null; then
              RECORD_COUNT=$(echo "$ALL_RECORDS" | jq 'length' 2>/dev/null || echo "0")
              
              if [[ "$RECORD_COUNT" -gt 0 ]] && [[ "$RECORD_COUNT" != "0" ]]; then
                echo "Found $RECORD_COUNT record(s) to delete..."
                
                # Build change batch
                CHANGE_BATCH=$(echo "$ALL_RECORDS" | jq '{
                  Changes: [.[] | {
                    Action: "DELETE",
                    ResourceRecordSet: ({
                      Name: .Name,
                      Type: .Type,
                      TTL: (.TTL // 300),
                      ResourceRecords: (.ResourceRecords // []),
                      AliasTarget: (.AliasTarget // empty)
                    } | with_entries(select(.value != null and .value != [])))
                  }]
                }' 2>/dev/null)
                
                if [[ -n "$CHANGE_BATCH" ]] && [[ "$CHANGE_BATCH" != "{}" ]]; then
                  TEMP_R53_CHANGE=$(mktemp)
                  echo "$CHANGE_BATCH" > "$TEMP_R53_CHANGE"
                  
                  CHANGE_ID=$(aws route53 change-resource-record-sets \
                    --hosted-zone-id "$ZONE_ID" \
                    --change-batch "file://$TEMP_R53_CHANGE" \
                    --query 'ChangeInfo.Id' \
                    --output text 2>/dev/null || echo "")
                  
                  rm -f "$TEMP_R53_CHANGE"
                  
                  if [[ -n "$CHANGE_ID" ]]; then
                    echo "âœ… DNS records deletion initiated (Change ID: $CHANGE_ID)"
                    echo "Waiting for deletion to complete..."
                    for i in {1..12}; do
                      STATUS=$(aws route53 get-change --id "$CHANGE_ID" --query 'ChangeInfo.Status' --output text 2>/dev/null || echo "PENDING")
                      if [[ "$STATUS" == "INSYNC" ]]; then
                        echo "âœ… DNS records deleted successfully"
                        break
                      fi
                      if [[ $i -eq 12 ]]; then
                        echo "âš ï¸  Deletion still in progress (waited 60s). Continuing..."
                      else
                        sleep 5
                      fi
                    done
                  else
                    echo "âš ï¸  Could not delete DNS records via batch operation"
                  fi
                fi
              else
                echo "âœ… No DNS records to delete"
              fi
            else
              echo "âš ï¸  jq not available - skipping Route 53 cleanup"
            fi
          fi

      # Terraform Destroy
      - name: Terraform Destroy
        if: ${{ inputs.action == 'destroy' }}
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
        run: |
          CLOUDS_INPUT="${{ inputs.target_cloud }}"
          TF_CLOUDS_LIST="[\"$(echo $CLOUDS_INPUT | sed 's/,/\",\"/g')\"]"
          
          TF_DESTROY_ARGS=(
            -auto-approve
            -var="target_clouds=$TF_CLOUDS_LIST"
            -var="deployment_mode=${{ inputs.deployment_mode }}"
            -var="app_image=${{ inputs.app_image }}"
            -var="domain_name=${{ inputs.domain_name }}"
            -var="enable_nat_gateway=${{ env.ENABLE_NAT_GATEWAY }}"
          )
          
          # Add GCP project ID if GCP is in target clouds
          if [[ "$CLOUDS_INPUT" == *"gcp"* ]] && [[ -n "${{ steps.gcp_project.outputs.project_id }}" ]]; then
            TF_DESTROY_ARGS+=(-var="gcp_project_id=${{ steps.gcp_project.outputs.project_id }}")
          fi
          
          terraform destroy "${TF_DESTROY_ARGS[@]}"

      # Post-destroy cleanup: Check for remaining Route 53 records
      - name: Post-Destroy Cleanup (Route 53 Records)
        if: ${{ inputs.action == 'destroy' && inputs.domain_name != '' && contains(inputs.target_cloud, 'aws') }}
        run: |
          echo "Checking for remaining Route 53 records after destroy..."
          
          # Extract apex domain (same logic as pre-destroy)
          DOMAIN="${{ inputs.domain_name }}"
          if [[ "$DOMAIN" =~ ^www\. ]]; then
            APEX_DOMAIN="${DOMAIN#www.}"
          elif [[ "$DOMAIN" =~ ^[^.]+\. ]]; then
            PARTS=$(echo "$DOMAIN" | tr '.' '\n' | wc -l)
            if [[ $PARTS -gt 2 ]]; then
              APEX_DOMAIN=$(echo "$DOMAIN" | sed 's/^[^.]*\.//')
            else
              APEX_DOMAIN="$DOMAIN"
            fi
          else
            APEX_DOMAIN="$DOMAIN"
          fi
          
          ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name "$APEX_DOMAIN" --query 'HostedZones[0].Id' --output text 2>/dev/null | cut -d'/' -f3)
          
          if [[ -n "$ZONE_ID" ]] && [[ "$ZONE_ID" != "None" ]]; then
            REMAINING_RECORDS=$(aws route53 list-resource-record-sets --hosted-zone-id "$ZONE_ID" \
              --max-items 1000 \
              --query "ResourceRecordSets[?Type != 'NS' && Type != 'SOA']" \
              --output json 2>/dev/null || echo "[]")
            
            if command -v jq &> /dev/null; then
              REMAINING_COUNT=$(echo "$REMAINING_RECORDS" | jq 'length' 2>/dev/null || echo "0")
              
              if [[ "$REMAINING_COUNT" -gt 0 ]] && [[ "$REMAINING_COUNT" != "0" ]]; then
                echo "âš ï¸  Found $REMAINING_COUNT remaining record(s)"
                echo "Cleaning up..."
                # Same cleanup logic as pre-destroy
                CLEANUP_BATCH=$(echo "$REMAINING_RECORDS" | jq '{
                  Changes: [.[] | {
                    Action: "DELETE",
                    ResourceRecordSet: ({
                      Name: .Name,
                      Type: .Type,
                      TTL: (.TTL // 300),
                      ResourceRecords: (.ResourceRecords // []),
                      AliasTarget: (.AliasTarget // empty)
                    } | with_entries(select(.value != null and .value != [])))
                  }]
                }' 2>/dev/null)
                
                if [[ -n "$CLEANUP_BATCH" ]] && [[ "$CLEANUP_BATCH" != "{}" ]]; then
                  TEMP_CLEANUP=$(mktemp)
                  echo "$CLEANUP_BATCH" > "$TEMP_CLEANUP"
                  CLEANUP_CHANGE_ID=$(aws route53 change-resource-record-sets \
                    --hosted-zone-id "$ZONE_ID" \
                    --change-batch "file://$TEMP_CLEANUP" \
                    --query 'ChangeInfo.Id' \
                    --output text 2>/dev/null || echo "")
                  rm -f "$TEMP_CLEANUP"
                  
                  if [[ -n "$CLEANUP_CHANGE_ID" ]]; then
                    echo "âœ… Remaining DNS records deleted"
                  fi
                fi
              else
                echo "âœ… No remaining DNS records"
              fi
            fi
          fi

      # Destroy resources ONLY if the pipeline fails (failure() returns true)
      - name: Terraform Destroy (on failure)
        if: ${{ inputs.action == 'deploy' && failure() }}
        env:
          ARM_CLIENT_ID: ${{ secrets.ARM_CLIENT_ID }}
          ARM_CLIENT_SECRET: ${{ secrets.ARM_CLIENT_SECRET }}
          ARM_SUBSCRIPTION_ID: ${{ secrets.ARM_SUBSCRIPTION_ID }}
          ARM_TENANT_ID: ${{ secrets.ARM_TENANT_ID }}
        run: |
          CLOUDS_INPUT="${{ inputs.target_cloud }}"
          TF_CLOUDS_LIST="[\"$(echo $CLOUDS_INPUT | sed 's/,/\",\"/g')\"]"
          
          TF_DESTROY_ARGS=(
            -auto-approve
            -var="target_clouds=$TF_CLOUDS_LIST"
            -var="deployment_mode=${{ inputs.deployment_mode }}"
            -var="app_image=${{ inputs.app_image }}"
            -var="domain_name=${{ inputs.domain_name }}"
            -var="enable_nat_gateway=${{ env.ENABLE_NAT_GATEWAY }}"
          )
          
          # Add GCP project ID if GCP is in target clouds
          if [[ "$CLOUDS_INPUT" == *"gcp"* ]] && [[ -n "${{ steps.gcp_project.outputs.project_id }}" ]]; then
            TF_DESTROY_ARGS+=(-var="gcp_project_id=${{ steps.gcp_project.outputs.project_id }}")
          fi
          
          terraform destroy "${TF_DESTROY_ARGS[@]}"

      - name: Publish Metrics
        if: always()
        run: |
          echo "=========================================="
          echo "ðŸ“Š Final Metrics"
          echo "=========================================="
          echo "Action: ${{ inputs.action }}"
          echo "Cloud: ${{ inputs.target_cloud }}"
          echo "Mode: ${{ inputs.deployment_mode }}"
          if [[ "${{ inputs.action }}" == "deploy" ]] && [[ -n "${{ steps.time_provisioning.outputs.duration }}" ]]; then
            echo "Provisioning Time: ${{ steps.time_provisioning.outputs.duration }} seconds"
          fi
          echo "Status: ${{ job.status }}"
